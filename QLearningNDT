import numpy as np
import pandas as pd
from collections import defaultdict

from google.colab import drive
drive.mount('/content/drive')

def Z(lambdas, X):
    result = 1
    for j in range(len(X.transpose())):
        result *= lambdas[j][0] * X[j]**2 + lambdas[j][1] * X[j] + lambdas[j][2]
    return result
    
def Y_star(X, lambdas, xis):
    xis = xis.reshape(2,)
    return xis[0] * Z(lambdas, X) + xis[1]
    
def cost(Y, Y_star):
    return np.sum(np.abs(Y - Y_star))/len(Y)
    
def Q_learning(X, Y, learning_rate, discount_factor, num_episodes, exploration_rate):
    rwd_list = []
    # Initialize the Q-table with zeros
    num_actions = 2 * X.shape[1]
    Q = defaultdict(lambda: np.zeros(num_actions))

    # Initialize the optimal reward
    optimal_reward = float('-inf')

    # Iterate over the episodes
    for episode in range(num_episodes):
        # Reset the environment
        lambdas = np.random.uniform(size=(4, 3))
        xis = np.random.uniform(size=(1, 2))
        Y_star_ = Y_star(X, lambdas, xis).squeeze()

        # Initialize the episode reward
        episode_reward = 0

        # Initialize state
        state = np.zeros(len(X), dtype=int)

        # Iterate over the steps in the episode
        for step in range(len(Y)):
            # Choose an action (i.e., update one element of the lambdas)
            if np.random.uniform(0, 1) < exploration_rate:
                # Exploration
                action_idx = np.random.randint(0, num_actions)
            else:
                # Exploitation
                Q_values = Q[tuple(state)]
                max_Q_value = np.max(Q_values)
                action_idx = np.argmax(Q_values)

            # Update the lambdas and xis based on the action index
            i = action_idx // 2
            j = action_idx % 2
            lambdas_ = np.copy(lambdas)
            lambdas_[i][j] += np.random.uniform(low=-1, high=1) * 0.1
            xis_ = np.copy(xis)
            xis_[0][j] += np.random.uniform(low=-0.1, high=0.1)

            # Compute the new Y_star value and the reward
            Y_star__ = Y_star(X, lambdas_, xis_).squeeze()
            next_state = np.round(Y_star__, 2).astype(int)

            # Check that each value in next_state is within the range of valid indices for X
            if np.all(next_state < num_bins):
                reward = -cost(Y.reshape(-1, 1), Y_star__)
                action = action_idx

                # Update the Q-value for the current state-action pair
                next_state = tuple([int(next_state[i]) for i in range(len(next_state))])
                Q[tuple(state)][action] += learning_rate * (reward + discount_factor * np.max(Q[tuple(next_state)]) - Q[tuple(state)][action])

                # Update the state and action
                state = next_state
                lambdas = lambdas_
                xis = xis_
                Y_star_ = Y_star__

                # Update the episode reward
                episode_reward += reward

        # Check if the episode reward is better than the optimal reward
        if episode_reward > optimal_reward:
            optimal_reward = episode_reward
            optimal_lambdas = lambdas
            optimal_xis = xis

    return Q, optimal_lambdas, optimal_xis, optimal_reward

# Load the data
df = pd.read_excel(r'/content/drive/MyDrive/Colab Notebooks/uas_ndt.xlsx')
df.head()

# Split the data into inputs and outputs
X = df[['attenuation_deviation','velocity_mean','beta_mean','intrinsic_mean']].values
Y = df['diameter_average'].values

# Set the hyperparameters
learning_rate = 0.1
discount_factor = 0.95
num_episodes = 5000
exploration_rate = 0.1
num_bins = 5

# Run the Q-learning algorithm
Q, opt_lambdas, opt_xis, reward = Q_learning(X, Y, learning_rate, discount_factor, num_episodes, exploration_rate)
